{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "fc02f6cd-c5a6-4ee7-bb26-0ded470d2e61",
      "metadata": {
        "id": "fc02f6cd-c5a6-4ee7-bb26-0ded470d2e61"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import tensorflow as tf\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "from tensorflow.keras import layers, models\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target  # Features and target\n",
        "\n",
        "# Shuffle and split data\n",
        "X, y = np.random.permutation(X), np.random.permutation(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "ff909690",
      "metadata": {
        "id": "ff909690"
      },
      "outputs": [],
      "source": [
        "#created the calibration set and the proper training set\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move tensors to the selected device (GPU or CPU)\n",
        "X_train_normal = X_train.clone()\n",
        "y_train_normal = y_train.clone()\n",
        "X_test_normal = X_test.clone()\n",
        "y_test_normal = y_test.clone()\n",
        "\n",
        "\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "X_train, X_full_calib, y_train, y_full_calib = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "56a8ae75",
      "metadata": {
        "id": "56a8ae75"
      },
      "outputs": [],
      "source": [
        "def repeat_sampling(num_epochs, sample_size):\n",
        "    def decorator(func):\n",
        "        def wrapper(X_full_calib, y_full_calib, *args, **kwargs):\n",
        "            all_coverages = []\n",
        "            N = X_full_calib.shape[0]\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                torch.manual_seed(epoch)  # Reproducible sampling\n",
        "                indices = torch.randperm(N)\n",
        "\n",
        "                calib_idx = indices[:sample_size]\n",
        "\n",
        "                X_calib = X_full_calib[calib_idx]\n",
        "                y_calib = y_full_calib[calib_idx]\n",
        "                coverage = func(\n",
        "                    X_calib, y_calib,\n",
        "                    X_test, y_test,\n",
        "                    *args, **kwargs\n",
        "                )\n",
        "                all_coverages.append(coverage)\n",
        "\n",
        "            mean_coverage = np.mean(all_coverages, axis=0)\n",
        "            return mean_coverage\n",
        "        return wrapper\n",
        "    return decorator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "a60ab015",
      "metadata": {
        "id": "a60ab015"
      },
      "outputs": [],
      "source": [
        "exp_iter = 100\n",
        "exp_sample_size = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "10c0aee4",
      "metadata": {
        "id": "10c0aee4"
      },
      "outputs": [],
      "source": [
        "def quantile_loss(y_pred, y_true, tau):\n",
        "    errors = y_true - y_pred\n",
        "    return torch.mean(torch.max(tau * errors, (tau - 1) * errors))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "5365e319",
      "metadata": {
        "id": "5365e319"
      },
      "outputs": [],
      "source": [
        "def get_coverage_quantile_regression(alpha):\n",
        "    input_dim = X_train.shape[1]\n",
        "\n",
        "    W_lower = torch.randn((input_dim, 1), requires_grad=True, dtype=torch.float32, device=device)\n",
        "    b_lower = torch.zeros(1, requires_grad=True, dtype=torch.float32, device=device)\n",
        "\n",
        "    W_upper = torch.randn((input_dim, 1), requires_grad=True, dtype=torch.float32, device=device)\n",
        "    b_upper = torch.zeros(1, requires_grad=True, dtype=torch.float32, device=device)\n",
        "\n",
        "    # Hyperparameters\n",
        "    taus = [alpha / 2, 1 - alpha / 2]  # Lower and Upper Quantiles\n",
        "    epochs = 500\n",
        "    lr = 0.01\n",
        "\n",
        "    # Optimizers\n",
        "    opt_lower = optim.Adam([W_lower, b_lower], lr=lr)\n",
        "    opt_upper = optim.Adam([W_upper, b_upper], lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Lower Quantile Model (tau = alpha/2)\n",
        "        opt_lower.zero_grad()\n",
        "        y_pred_lower = torch.matmul(X_train, W_lower) + b_lower\n",
        "        loss_lower = quantile_loss(y_pred_lower, y_train, taus[0])\n",
        "        loss_lower.backward()\n",
        "        opt_lower.step()\n",
        "\n",
        "        # Upper Quantile Model (tau = 1-alpha/2)\n",
        "        opt_upper.zero_grad()\n",
        "        y_pred_upper = torch.matmul(X_train, W_upper) + b_upper\n",
        "        loss_upper = quantile_loss(y_pred_upper, y_train, taus[1])\n",
        "        loss_upper.backward()\n",
        "        opt_upper.step()\n",
        "\n",
        "    @repeat_sampling(num_epochs=exp_iter, sample_size=exp_sample_size)\n",
        "    def calc_cvg(X_cal, y_cal, X_test, y_test):\n",
        "        # Move tensors to device\n",
        "        X_cal = X_cal.to(device)\n",
        "        y_cal = y_cal.to(device)\n",
        "\n",
        "        y_lower_pred = torch.matmul(X_cal, W_lower).detach()\n",
        "        y_upper_pred = torch.matmul(X_cal, W_upper).detach()\n",
        "\n",
        "        score_array = torch.maximum(y_lower_pred - y_cal, y_cal - y_upper_pred)\n",
        "        # Compute the quantile index\n",
        "        import math\n",
        "        n = score_array.shape[0]\n",
        "        quantile_index = math.ceil((n + 1) * (1 - alpha)) / n\n",
        "\n",
        "        # Compute the required quantile\n",
        "        q_hat = torch.quantile(score_array, quantile_index)\n",
        "        y_lower_pred_test = torch.matmul(X_test, W_lower).detach()\n",
        "        y_upper_pred_test = torch.matmul(X_test, W_upper).detach()\n",
        "\n",
        "        covered = (y_test >= (y_lower_pred_test - q_hat)) & (y_test <= (y_upper_pred_test + q_hat))\n",
        "\n",
        "        return covered.float().mean().item()\n",
        "\n",
        "    return calc_cvg(X_full_calib, y_full_calib)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "e06ff6dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e06ff6dd",
        "outputId": "f73a787d-5d3c-4139-9446-50249b88b571"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.8936498993635178)"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_coverage_quantile_regression(0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0672d13",
      "metadata": {
        "id": "e0672d13"
      },
      "source": [
        "### Uncertainty Estimate Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "8e94f747",
      "metadata": {
        "id": "8e94f747"
      },
      "outputs": [],
      "source": [
        "def get_coverage_uncertainty_model(alpha, X_test=X_test, y_test=y_test):\n",
        "    def build_point_predictor(input_shape):\n",
        "        model = models.Sequential([\n",
        "            layers.InputLayer(input_shape=(input_shape,)),\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.Dense(16, activation='relu'),\n",
        "            layers.Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        return model\n",
        "\n",
        "    def build_uncertainty_predictor(input_shape):\n",
        "        model = models.Sequential([\n",
        "            layers.InputLayer(input_shape=(input_shape,)),\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.Dense(16, activation='relu'),\n",
        "            layers.Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        return model\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy (and move to CPU if needed)\n",
        "    X_train_np = X_train.detach().cpu().numpy()\n",
        "    y_train_np = y_train.detach().cpu().numpy()\n",
        "    X_test_np = X_test.detach().cpu().numpy()\n",
        "    y_test_np = y_test.detach().cpu().numpy()\n",
        "\n",
        "    # Fit point predictor\n",
        "    point_model = build_point_predictor(X_train_np.shape[1])\n",
        "    point_model.fit(X_train_np, y_train_np, epochs=50, batch_size=16, verbose=0)\n",
        "\n",
        "    # Get absolute residuals\n",
        "    y_hat_train = point_model.predict(X_train_np, verbose=0)\n",
        "    residuals_train = np.abs(y_train_np - y_hat_train)\n",
        "\n",
        "    # Fit uncertainty model\n",
        "    uncertainty_model = build_uncertainty_predictor(X_train_np.shape[1])\n",
        "    uncertainty_model.fit(X_train_np, residuals_train, epochs=50, batch_size=16, verbose=0)\n",
        "\n",
        "    @repeat_sampling(num_epochs=exp_iter, sample_size=exp_sample_size)\n",
        "    def calc_cvg(X_cal, y_cal, X_test, y_test):\n",
        "        # Convert PyTorch tensors to NumPy\n",
        "        X_cal_np = X_cal.detach().cpu().numpy()\n",
        "        y_cal_np = y_cal.detach().cpu().numpy()\n",
        "        X_test_np = X_test.detach().cpu().numpy()\n",
        "        y_test_np = y_test.detach().cpu().numpy()\n",
        "\n",
        "        y_hat_calib = point_model.predict(X_cal_np, verbose=0)\n",
        "        residuals_calib = np.abs(y_cal_np - y_hat_calib)\n",
        "        uncertainty_scores = uncertainty_model.predict(X_cal_np, verbose=0)\n",
        "\n",
        "        scores = np.divide(residuals_calib, uncertainty_scores + 1e-8)\n",
        "        n = scores.shape[0]\n",
        "        quantile_index = math.ceil((n + 1) * (1 - alpha)) / n\n",
        "        q_hat = np.quantile(scores, quantile_index)\n",
        "\n",
        "        y_hat_test = point_model.predict(X_test_np, verbose=0)\n",
        "        uncertainty_scores_test = uncertainty_model.predict(X_test_np, verbose=0)\n",
        "\n",
        "        lower = y_hat_test - q_hat * uncertainty_scores_test\n",
        "        upper = y_hat_test + q_hat * uncertainty_scores_test\n",
        "        covered = (y_test_np >= lower) & (y_test_np <= upper)\n",
        "\n",
        "        return covered.mean()\n",
        "\n",
        "    return calc_cvg(X_full_calib, y_full_calib)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "2a5a2e08",
      "metadata": {
        "id": "2a5a2e08"
      },
      "outputs": [],
      "source": [
        "# get_coverage_uncertainty_model(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "7e854906",
      "metadata": {
        "id": "7e854906"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def get_coverage_xgboost(alpha):\n",
        "    # Train an XGBoost model for Point Prediction using GPU\n",
        "    def build_point_predictor():\n",
        "        model = xgb.XGBRegressor(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            tree_method='gpu_hist',           # GPU training\n",
        "            predictor='gpu_predictor',        # GPU prediction\n",
        "            verbosity=0\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    # Convert PyTorch tensors to CPU numpy arrays if needed\n",
        "    def to_numpy(x):\n",
        "        return x.detach().cpu().numpy() if hasattr(x, \"detach\") else x\n",
        "\n",
        "    X_train_np = to_numpy(X_train)\n",
        "    y_train_np = to_numpy(y_train)\n",
        "\n",
        "    # Train the Point Predictor Model\n",
        "    point_model = build_point_predictor()\n",
        "    point_model.fit(X_train_np, y_train_np)\n",
        "\n",
        "    # Get the Point Predictions and Residuals\n",
        "    y_hat_train = point_model.predict(X_train_np)\n",
        "    residuals_train = np.abs(y_train_np - y_hat_train)\n",
        "\n",
        "    # Uncertainty estimation: std deviation of tree leaf indices\n",
        "    leaf_preds_train = point_model.apply(X_train_np)\n",
        "    uncertainty_scores = np.std(leaf_preds_train, axis=1)\n",
        "\n",
        "    @repeat_sampling(num_epochs=exp_iter, sample_size=exp_sample_size)\n",
        "    def calc_cvg(X_cal, y_cal, X_test, y_test):\n",
        "        X_cal_np = to_numpy(X_cal)\n",
        "        y_cal_np = to_numpy(y_cal)\n",
        "        X_test_np = to_numpy(X_test)\n",
        "        y_test_np = to_numpy(y_test)\n",
        "\n",
        "        y_hat_calib = point_model.predict(X_cal_np)\n",
        "        residuals_calib = np.abs(y_cal_np - y_hat_calib)\n",
        "        leaf_preds_calib = point_model.apply(X_cal_np)\n",
        "        uncertainty_scores_calib = np.std(leaf_preds_calib, axis=1)\n",
        "\n",
        "        # Calculate scores and quantile\n",
        "        scores = np.divide(residuals_calib, uncertainty_scores_calib)\n",
        "        n = scores.shape[0]\n",
        "        quantile_index = math.ceil((n + 1) * (1 - alpha)) / n\n",
        "        q_hat = np.quantile(scores, quantile_index)\n",
        "\n",
        "        # Get predictions for test data\n",
        "        y_hat_test = point_model.predict(X_test_np)\n",
        "        leaf_preds_test = point_model.apply(X_test_np)\n",
        "        uncertainty_scores_test = np.std(leaf_preds_test, axis=1)\n",
        "\n",
        "        lower_bound = y_hat_test - q_hat * uncertainty_scores_test\n",
        "        upper_bound = y_hat_test + q_hat * uncertainty_scores_test\n",
        "\n",
        "        covered = (y_test_np >= lower_bound) & (y_test_np <= upper_bound)\n",
        "        return covered.mean()\n",
        "\n",
        "    return calc_cvg(X_full_calib, y_full_calib)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "ed277519",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed277519",
        "outputId": "8d385255-b9b3-4f08-8ee5-2d690744cfdd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.47755454545833914)"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_coverage_xgboost(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fe1yLqmn9ZUW",
      "metadata": {
        "id": "Fe1yLqmn9ZUW"
      },
      "outputs": [],
      "source": [
        "alpha = np.linspace(0.01, 0.5, 50)\n",
        "cvg_quantile_reg = []\n",
        "cvg_uncertainty_model = []\n",
        "\n",
        "for i in alpha:\n",
        "    cvg_quantile_reg.append(get_coverage_quantile_regression(i))\n",
        "    cvg_uncertainty_model.append(get_coverage_xgboost(i))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I20M0pYR-tnT",
      "metadata": {
        "id": "I20M0pYR-tnT"
      },
      "source": [
        "### Plot comparison of deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56KQBiid-yMf",
      "metadata": {
        "id": "56KQBiid-yMf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Calculate deviations from ideal coverage\n",
        "deviation_qr = np.array(cvg_quantile_reg) - (1 - alpha)\n",
        "deviation_uncertainty = np.array(cvg_uncertainty_model) - (1 - alpha)\n",
        "\n",
        "# Plot settings\n",
        "sns.set(style=\"whitegrid\", context=\"talk\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot deviations\n",
        "plt.plot(alpha, deviation_qr, label='Quantile Regression', marker='o', linewidth=2)\n",
        "plt.plot(alpha, deviation_uncertainty, label='Uncertainty Model', marker='s', linewidth=2)\n",
        "\n",
        "# Reference line at 0 (perfect coverage)\n",
        "plt.axhline(0, color='black', linestyle='--', linewidth=2, label='Ideal (0 deviation)')\n",
        "\n",
        "# Axis labels and title\n",
        "plt.xlabel('α (Significance Level)', fontsize=14)\n",
        "plt.ylabel('Coverage Deviation from (1 - α)', fontsize=14)\n",
        "plt.title('Deviation from Ideal Coverage vs. Significance Level', fontsize=16)\n",
        "\n",
        "# Custom ticks and grid\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Legend\n",
        "plt.legend(fontsize=12, loc='upper right')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Images/regression_coverage_deviation.png\", dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab1f0f94",
      "metadata": {
        "id": "ab1f0f94"
      },
      "source": [
        "### Jackknife, Jackknife+ Method - Full Conformal Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2dc01c9",
      "metadata": {
        "id": "c2dc01c9"
      },
      "outputs": [],
      "source": [
        "X_train_bias = torch.cat([X_train, torch.ones((X_train.shape[0], 1), device=device)], dim=1)\n",
        "X_test_bias = torch.cat([X_test, torch.ones((X_test.shape[0], 1), device=device)], dim=1)\n",
        "\n",
        "# Compute (X^T X)^-1 X^T y using torch\n",
        "A = X_train_bias.T @ X_train_bias            # X^T X\n",
        "A_inv = torch.inverse(A)                     # (X^T X)^-1\n",
        "w = A_inv @ X_train_bias.T @ y_train         # (X^T X)^-1 X^T y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7c8be88",
      "metadata": {
        "id": "f7c8be88"
      },
      "outputs": [],
      "source": [
        "score_array = torch.zeros(X_train_bias.shape[0], dtype=torch.float32)\n",
        "\n",
        "for i in range(len(X_train_bias)):\n",
        "    x = X_train_bias[i].unsqueeze(0)  # Shape: (1, D)\n",
        "    y = y_train[i]\n",
        "\n",
        "    # x: (1, D), x.T: (D, 1), A_inv: (D, D)\n",
        "    numerator = A_inv @ x.T @ x @ A_inv\n",
        "    denominator = 1 - (x @ A_inv @ x.T)\n",
        "\n",
        "    if denominator.item() == 0:\n",
        "        y_pred = x @ w\n",
        "        score_array[i] = torch.abs(y_pred - y)\n",
        "    else:\n",
        "        A_inv_new = A_inv + numerator / denominator\n",
        "        residual = y - x @ w\n",
        "        w_new = w - A_inv_new @ x.T @ residual\n",
        "        y_pred = x @ w_new\n",
        "        score_array[i] = torch.abs(y_pred - y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bfd83da",
      "metadata": {
        "id": "1bfd83da"
      },
      "source": [
        "Refer to Sherman-Morrison Update to add and remove efficiently in Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9d6417",
      "metadata": {
        "id": "8c9d6417"
      },
      "outputs": [],
      "source": [
        "def getCvgJackknife(alpha):\n",
        "    # Compute quantile index and value using PyTorch\n",
        "    index = math.ceil((len(score_array) + 1) * (1 - alpha)) / len(score_array)\n",
        "    q_hat = torch.quantile(score_array, torch.tensor(index, dtype=torch.float32))\n",
        "\n",
        "    # Predict with the linear model\n",
        "    y_test_pred = X_test_bias @ w\n",
        "\n",
        "    # Compute coverage\n",
        "    coverage = (y_test >= (y_test_pred - q_hat)) & (y_test <= (y_test_pred + q_hat))\n",
        "    coverage_percentage = coverage.float().mean() * 100\n",
        "\n",
        "    return coverage_percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9067e740",
      "metadata": {
        "id": "9067e740"
      },
      "outputs": [],
      "source": [
        "def getCvgJackknife_plus(alpha):\n",
        "    score_dict = [[] for _ in range(len(X_test_bias))]\n",
        "\n",
        "    for i in range(len(X_train_bias)):\n",
        "        x = X_train_bias[i].unsqueeze(0)  # Shape: (1, D)\n",
        "        y = y_train[i].unsqueeze(0)       # Shape: (1,)\n",
        "\n",
        "        denominator = 1 - (x @ A_inv @ x.T)\n",
        "        if denominator.item() == 0:\n",
        "            w_new = w\n",
        "        else:\n",
        "            numerator = A_inv @ x.T @ x @ A_inv\n",
        "            A_inv_new = A_inv + numerator / denominator\n",
        "            residual = y - x @ w\n",
        "            w_new = w - A_inv_new @ x.T @ residual\n",
        "\n",
        "        for j in range(len(X_test_bias)):\n",
        "            test_pred = X_test_bias[j].unsqueeze(0) @ w_new\n",
        "            score = torch.abs(test_pred - score_array[i])\n",
        "            score_dict[j].append(score.item())\n",
        "\n",
        "    coverage = 0\n",
        "    for i in range(len(X_test_bias)):\n",
        "        scores = torch.tensor(score_dict[i])\n",
        "        n = len(scores)\n",
        "\n",
        "        index_up = math.ceil((n + 1) * (1 - alpha)) / n\n",
        "        index_low = math.floor((n + 1) * alpha) / n\n",
        "\n",
        "        q_lower = torch.quantile(scores, q=index_low)\n",
        "        q_upper = torch.quantile(scores, q=index_up)\n",
        "\n",
        "        y_pred = X_test_bias[i].unsqueeze(0) @ w\n",
        "        y_actual = y_test[i]\n",
        "\n",
        "        lower_bound = y_pred - q_upper\n",
        "        upper_bound = y_pred + q_lower\n",
        "\n",
        "        if (y_actual >= lower_bound) and (y_actual <= upper_bound):\n",
        "            coverage += 1\n",
        "\n",
        "    coverage_percentage = (coverage / len(X_test_bias)) * 100\n",
        "    return coverage_percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3456c2f",
      "metadata": {
        "id": "b3456c2f"
      },
      "outputs": [],
      "source": [
        "# getCvgJackknife(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c6d4dc",
      "metadata": {
        "id": "e4c6d4dc"
      },
      "outputs": [],
      "source": [
        "# getCvgJackknife_plus(0.1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
